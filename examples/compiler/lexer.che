#load("std:preload")
#load("std:io/file")

#load("token")
#load("string_database")


struct Lexer {
    text        : String
    location    : TokenLocation
    peek        : Option(Token)
    string_db   : ref StringDatabase

    
    // interned keywords
    KwReturn    : string
    KwNew       : string
    KwRef       : string
    KwFn        : string
    KwStruct    : string
    KwEnum      : string
    KwImpl      : string
    KwConst     : string
    KwLet       : string
    KwTypedef   : string
    KwIf        : string
    KwElse      : string
    KwFor       : string
    KwWhile     : string
    KwAnd       : string
    KwOr        : string
    KwTrue      : string
    KwFalse     : string
    KwNull      : string
    KwUsing     : string
    KwDefer     : string
    KwMatch     : string
    KwBreak     : string
    KwContinue  : string
    KwTrait     : string
    KwCast      : string
}

fn is_ident_begin(c: char) -> bool {
    return (c >= 'a' and c <= 'z') or (c >= 'A' and c <= 'Z') or (c == '_')
}

fn is_ident_char(c: char) -> bool {
    return is_ident_begin(c) or (c >= '0' and c <= '9')
}

fn is_digit(c: char) -> bool {
    return c >= '0' and c <= '9'
}

fn is_hex_digit(c: char) -> bool {
    return (c >= '0' and c <= '9') or (c >= 'a' and c <= 'f') or (c >= 'A' and c <= 'F')
}

fn is_binary_digit(c: char) -> bool {
    return c >= '0' and c <= '1'
}

enum LexerNumberState {
    Error
    Init
    Z
    X
    B
    DecDigit
    BinDigit
    HexDigit
    Done
    FloatPoint
    FloatDigit
}

impl Lexer {
    fn from_raw_string(content: string, string_db: ref StringDatabase) -> Lexer {
        return from_string(String::from_string(content), string_db)
    }

    fn from_string(content: String, string_db: ref StringDatabase) -> Lexer {
        return new {
            text = content
            location = new {
                file = "string"
                line = 1
            }
            peek = None
            string_db = string_db

            KwReturn    = string_db.intern("return")
            KwNew       = string_db.intern("new")
            KwRef       = string_db.intern("ref")
            KwFn        = string_db.intern("fn")
            KwStruct    = string_db.intern("struct")
            KwEnum      = string_db.intern("enum")
            KwImpl      = string_db.intern("impl")
            KwConst     = string_db.intern("const")
            KwLet       = string_db.intern("let")
            KwTypedef   = string_db.intern("typedef")
            KwIf        = string_db.intern("if")
            KwElse      = string_db.intern("else")
            KwFor       = string_db.intern("for")
            KwWhile     = string_db.intern("while")
            KwAnd       = string_db.intern("and")
            KwOr        = string_db.intern("or")
            KwTrue      = string_db.intern("true")
            KwFalse     = string_db.intern("false")
            KwNull      = string_db.intern("null")
            KwUsing     = string_db.intern("using")
            KwDefer     = string_db.intern("defer")
            KwMatch     = string_db.intern("match")
            KwBreak     = string_db.intern("break")
            KwContinue  = string_db.intern("continue")
            KwTrait     = string_db.intern("trait")
            KwCast      = string_db.intern("cast")
        }
    }

    fn from_file(filename: string, string_db: ref StringDatabase) -> Result(Lexer, String) {
        return match load_file(filename) {
            Ok($content) -> {
                let l = Lexer::from_string(content, string_db)
                l.location.file = filename
                Ok(l)
            }
            Err($msg) -> Err(msg)
        }
    }

    fn peek_token(ref Self) -> Token {
        return match peek {
            Some($tok) -> tok

            None -> {
                let tok = next_token()
                peek = Some(tok)
                tok
            }
        }
    }

    fn next_token(ref Self) -> Token {
        match peek {
            Some($t) -> {
                peek = None
                return t
            }
        }

        match skip_newlines_and_comments() {
            Some($loc) -> {
                loc.end = loc.index
                return new Token {
                    ttype = TokenType.NewLine
                    location = loc
                    suffix = None
                    data = TokenData.None
                }
            }
        }

        return read_token()
    }

    fn read_token(ref Self) -> Token {
        let token = new Token {
            ttype = TokenType.EOF
            data = TokenData.None
            location = location
            suffix = None
        }
        token.location.end = token.location.index

        if location.index >= text.length {
            return token
        }

        let curr = peek_char(0)
        let next = peek_char(1)

        match curr {
            '=' if next == '=' -> simple_token(token, TokenType.DoubleEqual, 2)
            '!' if next == '=' -> simple_token(token, TokenType.NotEqual, 2)
            '<' if next == '=' -> simple_token(token, TokenType.LessEqual, 2)
            '<' if next == '<' -> simple_token(token, TokenType.LessLess, 2)
            '>' if next == '=' -> simple_token(token, TokenType.GreaterEqual, 2)
            ':' if next == ':' -> simple_token(token, TokenType.DoubleColon, 2)
            '-' if next == '>' -> simple_token(token, TokenType.Arrow, 2)
            '+' if next == '=' -> simple_token(token, TokenType.AddEq, 2)
            '-' if next == '=' -> simple_token(token, TokenType.SubEq, 2)
            '*' if next == '=' -> simple_token(token, TokenType.MulEq, 2)
            '/' if next == '=' -> simple_token(token, TokenType.DivEq, 2)
            '%' if next == '=' -> simple_token(token, TokenType.ModEq, 2)
            ':' -> simple_token(token, TokenType.Colon)
            ';' -> simple_token(token, TokenType.Semicolon)
            '.' -> simple_token(token, TokenType.Period)
            '=' -> simple_token(token, TokenType.Equal)
            '(' -> simple_token(token, TokenType.OpenParen)
            ')' -> simple_token(token, TokenType.ClosingParen)
            '{' -> simple_token(token, TokenType.OpenBrace)
            '}' -> simple_token(token, TokenType.ClosingBrace)
            '[' -> simple_token(token, TokenType.OpenBracket)
            ']' -> simple_token(token, TokenType.ClosingBracket)
            ',' -> simple_token(token, TokenType.Comma)
            '&' -> simple_token(token, TokenType.Ampersand)
            '*' -> simple_token(token, TokenType.Asterisk)
            '/' -> simple_token(token, TokenType.ForwardSlash)
            '+' -> simple_token(token, TokenType.Plus)
            '%' -> simple_token(token, TokenType.Percent)
            '-' -> simple_token(token, TokenType.Minus)
            '<' -> simple_token(token, TokenType.Less)
            '>' -> simple_token(token, TokenType.Greater)
            '!' -> simple_token(token, TokenType.Bang)
            '^' -> simple_token(token, TokenType.Caret)

            '"' -> {
                parse_string_literal(token, TokenType.StringLiteral, '"')
                parse_suffix(token)
            }
            '`'' -> {
                parse_string_literal(token, TokenType.CharLiteral, '`'')
                parse_suffix(token)
            }

            // identifiers and keywords
            '$' -> {
                location.index += 1
                parse_identifier(token, TokenType.DollarIdentifier)
            }
            '#' ->  {
                location.index += 1
                parse_identifier(token, TokenType.HashIdentifier)
            }
            '@' ->  {
                location.index += 1
                parse_identifier(token, TokenType.AtSignIdentifier)
            }
            $x if is_ident_begin(x) -> {
                parse_identifier(token, TokenType.Identifier)
                check_keywords(token)
                // check_keywords_raw(token)
            }

            // number literal
            $x if is_digit(x) -> {
                parse_number_literal(token)
                parse_suffix(token)
            }

            $_ -> {
                token.ttype = TokenType.Unknown
                location.index += 1
            }
        }

        token.location.end = location.index

        return token
    }

    fn parse_number_literal(ref Self, token: ref Token) {
        token.ttype = TokenType.NumberLiteral
        let base = 10
        let str = {
            let const cap = 64
            let raw = @alloca(char, cap)
            let str = String::from_raw_ptr(raw, cap)
            str
        }

        let isFloat = false

        use LexerNumberState

        let state = Init
        while location.index < text.length {
            let c = peek_char(0)
            match state {
                Error -> {break}
                Done  -> {break}

                Init  -> {
                    if c == '0' {
                        str += c
                        state = Z
                    } else if is_digit(c) {
                        str += c
                        state = DecDigit
                    }
                }

                Z -> match c {
                    'x' -> {
                        base = 16
                        str.resize(0)
                        state = X
                    }
                    'b' -> {
                        base = 2
                        str.resize(0)
                        state = B
                    }
                    '.' -> {
                        str += c
                        state = FloatPoint
                    }
                    $x if is_digit(x) -> {
                        str += x
                        state = DecDigit
                    }
                    $_ -> {
                        state = Done
                    }
                }

                DecDigit -> match c {
                    '.' -> {
                        str += c
                        state = FloatPoint
                    }
                    $x if is_digit(x) -> {
                        str += c
                    }
                    $_ -> {
                        state = Done
                    }
                }

                FloatPoint -> {
                    isFloat = true
                    if is_digit(c) {
                        str += c
                        state = FloatDigit
                    } else {
                        state = Error
                    }
                }

                FloatDigit -> match c {
                    $c if is_digit(c) -> {
                        str += c
                    }
                    $_ -> {
                        state = Done
                    }
                }

                X -> match c {
                    $c if is_hex_digit(c) -> {
                        str += c
                        state = HexDigit
                    }
                    $_ -> {
                        state = Error
                    }
                }

                HexDigit -> match c {
                    $c if is_hex_digit(c) -> {
                        str += c
                    }
                    $_ -> {
                        state = Done
                    }
                }

                B -> match c {
                    $c if is_binary_digit(c) -> {
                        str += c
                        state = BinDigit
                    }
                    $_ -> {
                        state = Error
                    }
                }

                BinDigit -> match c {
                    $c if is_binary_digit(c) -> {
                        str += c
                    }
                    $_ -> {
                        state = Done
                    }
                }
            }

            match state {
                Done -> {}
                $_   -> {
                    location.index += 1
                }
            }
        }

        match state {
            Error -> {
                // TODO: report error
                token.ttype = TokenType.Unknown
                return
            }
        }

        str += '`0'

        if isFloat {
            let d = c_strtod(str.get_raw(), null)
            token.data = TokenData.Double(d)
        } else {
            let i = c_strtol(str.get_raw(), null, cast base)
            token.data = TokenData.Integer(i)
        }
    }

    fn parse_suffix(ref Self, token: ref Token) {
        if is_ident_begin(peek_char(0)) {
            let start = location.index
            while location.index < text.length and is_ident_char(peek_char(0)) {
                location.index += 1
            }

            token.suffix = Some(text.sliceFL(start, location.index - start))
        }
    }

    fn simple_token(ref Self, token: ref Token, ttype: TokenType, len: int = 1) {
        token.ttype = ttype
        location.index += len
    }

    fn parse_identifier(ref Self, token: ref Token, ttype: TokenType) {
        token.ttype = ttype
        let start = location.index

        while location.index < text.length and is_ident_char(peek_char(0)) {
            location.index += 1
        }

        let str = text.sliceFL(start, location.index - start)
        token.data = TokenData.String(string_db.intern(str))
    }

    fn parse_string_literal(ref Self, token: ref Token, ttype: TokenType, end: char) {
        token.ttype = ttype
        location.index += 1
        let start = location.index

        let foundEnd = false
        while location.index < text.length {
            let c = peek_char(0)
            location.index += 1

            if c == end {
                foundEnd = true
                break
            } else if c == '``' {
                if location.index >= text.length {
                    // TODO: report error
                    break
                }

                location.index += 1
            }

            if c == '`n' {
                location.line += 1
                location.line_start = location.index
            }
        }

        if !foundEnd {
            // TODO: report
        }

        let str = text.sliceFL(start, location.index - start - 1)
        token.data = TokenData.String(string_db.intern(str))
    }

    fn skip_newlines_and_comments(ref Self) -> loc: Option(TokenLocation) {
        loc = None

        while location.index < text.length {
            let c = peek_char(0)
            let next = peek_char(1)

            if c == '/' and next == '*' {
                parse_multi_line_comment()
            } else if c == '/' and next == '/' {
                parse_single_line_comment()
            } else if c == ' ' or c == '`t' {
                location.index += 1
            } else if c == '`r' {
                location.index += 1
            } else if c == '`n' {
                match loc {
                    None -> {
                        loc = Some(location)
                    }
                }

                location.line += 1
                location.index += 1
                location.line_start = location.index
            } else {
                break
            }
        }
    }

    fn peek_char(ref Self, offset: int) -> char {
        let index = location.index + offset
        if index >= text.length {
            return '`0'
        }

        return text[index]
    }

    fn parse_multi_line_comment(ref Self) {
        let level = 0
        while location.index < text.length, location.index += 1 {
            let curr = peek_char(0)
            let next = peek_char(1)

            if curr == '/' and next == '*' {
                location.index += 1
                level += 1
            } else if curr == '*' and next == '/' {
                location.index += 1
                level -= 1

                if level == 0 {
                    break
                }
            } else if curr == '\n' {
                location.line += 1
                location.line_start = location.index
            }
        }
    }

    fn parse_single_line_comment(ref Self) {
        while location.index < text.length {
            if peek_char(0) == '`n' {
                break
            }

            location.index += 1
        }
    }

    fn check_keywords(ref Self, token: ref Token) {
        match token.data {
            TokenData.String($str) -> {
                token.ttype =
                    if      str == KwReturn     then TokenType.KwReturn
                    else if str == KwNew        then TokenType.KwNew
                    else if str == KwRef        then TokenType.KwRef
                    else if str == KwFn         then TokenType.KwFn
                    else if str == KwStruct     then TokenType.KwStruct
                    else if str == KwEnum       then TokenType.KwEnum
                    else if str == KwImpl       then TokenType.KwImpl
                    else if str == KwConst      then TokenType.KwConst
                    else if str == KwLet        then TokenType.KwLet
                    else if str == KwTypedef    then TokenType.KwTypedef
                    else if str == KwIf         then TokenType.KwIf
                    else if str == KwElse       then TokenType.KwElse
                    else if str == KwFor        then TokenType.KwFor
                    else if str == KwWhile      then TokenType.KwWhile
                    else if str == KwAnd        then TokenType.KwAnd
                    else if str == KwOr         then TokenType.KwOr
                    else if str == KwTrue       then TokenType.KwTrue
                    else if str == KwFalse      then TokenType.KwFalse
                    else if str == KwNull       then TokenType.KwNull
                    else if str == KwUsing      then TokenType.KwUsing
                    else if str == KwDefer      then TokenType.KwDefer
                    else if str == KwMatch      then TokenType.KwMatch
                    else if str == KwBreak      then TokenType.KwBreak
                    else if str == KwContinue   then TokenType.KwContinue
                    else if str == KwTrait      then TokenType.KwTrait
                    else if str == KwCast       then TokenType.KwCast
                    else                             token.ttype
            }
        }
    }
}
