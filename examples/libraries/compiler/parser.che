use import std.array
use import std.fiber

mem :: import std.mem.allocator
io  :: import std.io
fmt :: import std.fmt

use import logging.logger

use import ast
use import compiler
use import config
use import error_handler
use import lexer
use import value

#export_scope

Parser :: struct {
    lexer           : &Lexer
    error_handler   : ^ErrorHandler
    allocator       : ^mem.Allocator

    _next_id        := 1
}

impl Parser {
    debug_log :: (&Self, msg: string, args: []^any = []) {
        if const DEBUG_ENABLED { if DEBUG_PARSER {
            str := fmt.format(msg, args)
            location := lexer.current_location()
            g_logger.log("[{}] [Parser] {}:{}:{}: {}", [Fiber.user_data(FiberContext).thread, location.file, location.line, location.column, str])
        }}
    }

    new :: (lexer: &Lexer, error_handler: ^ErrorHandler, allocator: ^mem.Allocator, first_id: int) -> Self {
        return Parser(
            lexer           = lexer
            error_handler   = error_handler
            allocator       = allocator
            _next_id        = first_id
        )
    }

    next_id :: (&Self) -> int {
        return _next_id <- _next_id + 1
    }

    allocate :: (&Self, value: $T) -> ^T {
        result := mem.alloc(T, allocator)
        *result = value
        return result
    }

    try :: (code: Code) #macro {
        value := @insert(code)
        if value == null {
            return null
        }
        value
    }

    parse_node :: (&Self) -> ^AstNode {
        lexer.skip_whitespace()
        if check_token(.EOF) {
            return null
        }
        expr := parse_expression(true, true)
        consume(.NewLine)
        return expr
    }

    parse_expression :: (&Self, allow_comma: bool, allow_function: bool) -> ^AstNode {
        if check_token(.KwPub) {
            lexer.next_token()
            mutable := if check_token(.KwMut) {
                lexer.next_token()
                true
            } else false
            return parse_declaration(public=true, mutable=mutable, allow_comma=allow_comma, allow_function=allow_function)
        } else if check_token(.KwMut) {
            lexer.next_token()
            return parse_declaration(public=false, mutable=true, allow_comma=allow_comma, allow_function=allow_function)
        }

        expr := parse_comma_expression(allow_comma, allow_function)

        if check_token(.Equal) {
            lexer.next_token()
            lexer.skip_whitespace()
            value_expr := parse_comma_expression(allow_comma, allow_function)

            return allocate(AstAssignment(
                id          = next_id()
                location    = expr.location.to(value_expr.location)
                pattern     = &*expr
                value_expr  = &*value_expr
            ))
        } else if check_token(.Colon) {
            lexer.next_token()
            lexer.skip_whitespace()

            type_expr : ^AstNode = if check_token(.Equal) or check_token(.Colon) {
                null
            } else {
                parse_comma_expression(allow_comma, allow_function)
            }

            if check_token(.Colon) {
                lexer.next_token()
                lexer.skip_whitespace()
                value_expr := parse_comma_expression(allow_comma, allow_function)

                return allocate(AstConstDecl(
                    id          = next_id()
                    location    = expr.location.to(value_expr.location)
                    pattern     = &*expr
                    type_expr   = type_expr
                    value_expr  = &*value_expr
                ))
            } else if check_token(.Equal) {
                lexer.next_token()
                lexer.skip_whitespace()
                value_expr := parse_comma_expression(allow_comma, allow_function)

                return allocate(AstDecl(
                    id          = next_id()
                    location    = expr.location.to(value_expr.location)
                    pattern     = expr
                    type_expr   = type_expr
                    value_expr  = value_expr
                    mutable     = false
                ))
            } else {
                return allocate(AstDecl(
                    id          = next_id()
                    location    = expr.location.to(type_expr.location)
                    pattern     = expr
                    type_expr   = type_expr
                    value_expr  = null
                    mutable     = false
                ))
            }
        }
        
        return expr
    }

    parse_declaration :: (&Self, public: bool, mutable: bool, allow_comma: bool, allow_function: bool) -> ^AstNode {
        pattern := try(parse_comma_expression(allow_comma, allow_function))

        consume(.Colon)
        lexer.skip_whitespace()

        type_expr : ^AstNode = if check_token(.Equal) or check_token(.Colon) {
            null
        } else {
            try(parse_comma_expression(allow_comma, allow_function))
        }

        if check_token(.Equal) {
            lexer.next_token()
            lexer.skip_whitespace()
            value_expr := try(parse_comma_expression(allow_comma, allow_function))

            return allocate(AstDecl(
                id          = next_id()
                location    = pattern.location.to(value_expr.location)
                pattern     = pattern
                type_expr   = type_expr
                value_expr  = value_expr
                public      = public
                mutable     = mutable
            ))
        } else if check_token(.Colon) {
            lexer.next_token()
            lexer.skip_whitespace()
            value_expr := try(parse_comma_expression(allow_comma, allow_function))

            if mutable {
                error_handler.report_error_at(pattern.location, "Constant declaration can't be mutable")
            }

            return allocate(AstConstDecl(
                id          = next_id()
                location    = pattern.location.to(value_expr.location)
                pattern     = &*pattern
                type_expr   = type_expr
                value_expr  = &*value_expr
                public      = public
            ))
        } else {
            return allocate(AstDecl(
                id          = next_id()
                location    = pattern.location.to(type_expr.location)
                pattern     = pattern
                type_expr   = type_expr
                value_expr  = null
                public      = public
                mutable     = mutable
            ))
        }
        
        return pattern
    }

    parse_comma_expression :: (&Self, allow_comma: bool, allow_function: bool) -> ^AstNode {
        expr := parse_binary(allow_comma, allow_function)

        if allow_comma and check_token(.Comma) {
            values := Array[^AstNode].create()
            values.add(expr)
            location := expr.location

            while check_token(.Comma) {
                lexer.next_token()
                lexer.skip_whitespace()
                values.add(parse_binary(true, allow_function))
                location = location.to(values.peek_last().location)
            }

            id := next_id()
            expr = allocate(AstTuple(
                id          = id
                location    = location
                values      = values 
            ))
        }

        return expr
    }

    parse_binary :: (&Self, allow_comma: bool, allow_function: bool, precedence: int = int.min, left_associative: bool = true) -> ^AstNode {
        expr := parse_unary(allow_comma, allow_function)

        loop {
            next := lexer.peek_token()

            op, pre, left_asso := match next.typ {
                .ReverseArrow   -> AstBinary.BinOp.Move,         3, false
                .Pipe           -> AstBinary.BinOp.Pipe,         3, true

                .KwAnd          -> AstBinary.BinOp.And,          4, true
                .KwOr           -> AstBinary.BinOp.Or,           4, true

                .Less           -> AstBinary.BinOp.Less,         5, true
                .LessEqual      -> AstBinary.BinOp.LessEq,       5, true
                .Greater        -> AstBinary.BinOp.Greater,      5, true
                .GreaterEqual   -> AstBinary.BinOp.GreaterEq,    5, true
                .DoubleEqual    -> AstBinary.BinOp.Equal,        5, true
                .NotEqual       -> AstBinary.BinOp.NotEqual,     5, true

                .PeriodPeriod   -> AstBinary.BinOp.Range,        6, true
                .PeriodPeriodEq -> AstBinary.BinOp.RangeIncl,    6, true

                .Plus           -> AstBinary.BinOp.Add,          7, true
                .Minus          -> AstBinary.BinOp.Sub,          7, true

                .Asterisk       -> AstBinary.BinOp.Mul,          8, true
                .ForwardSlash   -> AstBinary.BinOp.Div,          8, true
                .Percent        -> AstBinary.BinOp.Mod,          8, true

                _ -> {
                    return expr
                }
            }

            if pre == precedence and left_asso != left_associative {
                error_handler.report_error_at(next.location, "Ambigious operator precedence. Use parenthesis to disambiguate")
                return expr
            }

            if (left_asso and pre <= precedence) or (!left_asso and pre < precedence) {
                return expr
            }

            lexer.next_token()
            lexer.skip_whitespace()

            id := next_id()
            right := parse_binary(allow_comma, allow_function, pre, left_asso)
            location := expr.location.to(right.location)
            expr = allocate(AstBinary(
                id          = id
                location    = location
                operator    = op
                left        = &*expr
                right       = &*right
            ))
        }

        @assert(false)
        return null
    }

    parse_unary :: (&Self, allow_comma: bool, allow_function: bool) -> ^AstNode {
        next := lexer.peek_token()
        match next.typ {
            .Minus -> {
                beg := lexer.next_token().location
                sub := parse_unary(allow_comma, allow_function)
                return allocate(AstUnary(
                    id          = next_id()
                    location    = beg.to(sub.location)
                    operator    = .Neg
                    sub         = &*sub
                ))
            }
            .Bang -> {
                debug_log("unary bang {}", [lexer.peek_token().typ])
                beg := lexer.next_token().location
                sub := parse_unary(allow_comma, allow_function)
                return allocate(AstUnary(
                    id          = next_id()
                    location    = beg.to(sub.location)
                    operator    = .Not
                    sub         = &*sub
                ))
            }
            .Ampersand -> {
                beg := lexer.next_token().location

                mutable := if check_token(.KwMut) {
                    lexer.next_token()
                    true
                } else {
                    false
                }

                sub := parse_unary(allow_comma, allow_function)
                return allocate(AstUnary(
                    id          = next_id()
                    location    = beg.to(sub.location)
                    operator    = if mutable then .RefMut else .Ref
                    sub         = &*sub
                ))
            }
            .Hat -> {
                beg := lexer.next_token().location

                mutable := if check_token(.KwMut) {
                    lexer.next_token()
                    true
                } else {
                    false
                }

                sub := parse_unary(allow_comma, allow_function)
                return allocate(AstUnary(
                    id          = next_id()
                    location    = beg.to(sub.location)
                    operator    = if mutable then .PtrMut else .Ptr
                    sub         = &*sub
                ))
            }
            .Asterisk -> {
                beg := lexer.next_token().location
                sub := parse_unary(allow_comma, allow_function)
                return allocate(AstUnary(
                    id          = next_id()
                    location    = beg.to(sub.location)
                    operator    = .Deref
                    sub         = &*sub
                ))
            }
        }
        return parse_post_unary(allow_comma, allow_function)
    }

    parse_post_unary :: (&Self, allow_comma: bool, allow_function: bool) -> ^AstNode {
        debug_log("parse_post_unary")
        expr := try(parse_atomic_expression(allow_comma, allow_function))

        loop {
            match lexer.peek_token().typ {
                .OpenParen -> {
                    args, beg, end := parse_argument_list(.OpenParen, .ClosingParen)
                    expr = allocate(AstCall(
                        id          = next_id()
                        location    = expr.location.to(end)
                        function    = &*expr
                        arguments   = args
                    ))
                }

                .OpenBracket -> {
                    args, beg, end := parse_argument_list(.OpenBracket, .ClosingBracket)
                    if args.count() == 0 {
                        error_handler.report_error_at(beg, "Missing argument")
                        return null
                    }
                    index := *args[0]
                    expr = allocate(AstIndex(
                        id          = next_id()
                        location    = expr.location.to(end)
                        sub         = &*expr
                        arguments   = args
                        index       = index
                    ))
                }

                .Period -> {
                    lexer.next_token()
                    name := try(parse_identifier())
                    expr = allocate(AstDot(
                        id          = next_id()
                        location    = expr.location.to(name.location)
                        sub         = expr
                        name        = &*name
                    ))
                }

                _ -> {
                    debug_log("return expr")
                    return expr
                }
            }
        }

        @assert(false)
        return null
    }

    parse_bracketed :: (&Self) -> ^AstNode {
        beg := consume(.OpenBracket).location

        values := Array[^AstNode].create()
        loop {
            next := *lexer.peek_token()
            if next.typ == .ClosingBracket or next.typ == .EOF {
                break
            }

            expr := parse_expression(false, true)

            if expr != null {
                values.add(expr)
            }

            next = *lexer.peek_token()
            if next.typ == .ClosingBracket or next.typ == .EOF {
                break
            }

            consume(.Comma)
            lexer.skip_whitespace()
        }

        end := consume(.ClosingBracket).location

        if is_expr_token() {
            if values.count() > 1 {
                error_handler.report_error_at(beg.to(end), "Too many values in array type expression")
            }

            target := try(parse_unary(false, false))

            count : ^AstNode = null
            if values.count() > 0 {
                count = *values[0]
            }
            return allocate(AstArrayType(
                id          = next_id()
                location    = beg.to(target.location)
                count       = count
                target      = &*target
            ))
        }


        return allocate(AstArray(
            id          = next_id()
            location    = beg.to(end)
            values      = values
        ))
    }


    parse_block :: (&Self) -> ^AstNode {
        id := next_id()

        children := Array[^AstNode].create()

        location := consume(.OpenBrace).location
        lexer.skip_whitespace()

        loop {
            next := *lexer.peek_token()
            if next.typ == .ClosingBrace or next.typ == .EOF {
                break
            }

            child := try(parse_expression(true, true))
            children.add(child)

            next = *lexer.peek_token()
            if next.typ == .ClosingBrace or next.typ == .EOF {
                break
            }
            consume(.NewLine)
        }

        lexer.skip_whitespace()
        end := consume(.ClosingBrace).location

        location.end_line = end.end_line
        
        return allocate(AstBlock(
            id          = id
            location    = location
            sub_scope   = null
            children    = children
        ))
    }

    parse_match :: (&Self, allow_comma: bool) -> ^AstNode {
        location := consume(.KwMatch).location

        lexer.skip_whitespace()
        value_expr : ^AstNode = if check_token(.OpenBrace) then null else parse_expression(allow_comma, false)

        lexer.skip_whitespace()
        consume(.OpenBrace)

        cases := Array[^AstMatchCase].create()
        loop {
            lexer.skip_whitespace()
            next := lexer.peek_token()
            if next.typ == .ClosingBrace or next.typ == .EOF {
                break
            }

            pattern := try(parse_expression(true, false))

            lexer.skip_whitespace()
            condition : ^AstNode = if check_token(.KwIf) {
                lexer.next_token()
                try(parse_expression(true, false))
            } else {
                null
            }

            lexer.skip_whitespace()
            consume(.Arrow)

            body := try(parse_expression(true, false))

            cases.add(allocate(AstMatchCase(
                pattern     = &*pattern
                condition   = condition
                body        = &*body
            )))
        }

        end := consume(.ClosingBrace).location

        return allocate(AstMatch(
            id          = next_id()
            location    = location.to(end)
            value_expr  = value_expr
            cases       = cases
        ))
    }

    parse_if :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        location := consume(.KwIf).location

        lexer.skip_whitespace()
        condition := parse_expression(false, false)

        true_case := if check_token(.KwThen) {
            lexer.next_token()
            parse_expression(allow_comma, false)
        } else {
            parse_block()
        }

        location = location.to(true_case.location)

        false_case : ^AstNode = if check_token(.KwElse) {
            lexer.next_token()
            lexer.skip_whitespace()
            expr := parse_expression(allow_comma, false)
            location = location.to(expr.location)
            expr
        } else {
            null
        }

        return allocate(AstIf(
            id          = id
            location    = location
            condition   = &*condition
            true_case   = &*true_case
            false_case  = false_case
        ))
    }

    parse_loop :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()
        location := consume(.KwLoop).location
        body := parse_expression(allow_comma, false)
        location = location.to(body.location)
        return allocate(AstLoop(
            id          = id
            location    = location
            body        = &*body
        ))
    }

    parse_for :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        location := consume(.KwFor).location

        // @todo: parse names

        it_pattern : ^AstNode = null
        index_name : ^AstIdentifier = null
        collection : ^AstNode = null

        expr := parse_expression(false, false)

        if check_token(.Comma) {
            it_pattern = expr
            lexer.next_token()
            lexer.skip_whitespace()
            index_name = try(parse_identifier())

            lexer.skip_whitespace()
            consume(.KwIn)
            collection = parse_expression(false, false)
        } else if check_token(.KwIn) {
            it_pattern = expr
            lexer.next_token()
            lexer.skip_whitespace()
            collection = parse_expression(false, false)
        } else {
            collection = expr
        }

        body := parse_block()

        location = location.to(body.location)
        return allocate(AstFor(
            id          = id
            location    = location
            it_pattern  = it_pattern
            index_name  = index_name
            collection  = &*collection
            body        = &*body
        ))
    }

    parse_break :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        location := consume(.KwBreak).location

        // @todo: parse optional label and expression

        // location.end_line = body.location.end_line
        return allocate(AstBreak(
            id          = id
            location    = location
            label       = null
            value_expr  = null
        ))
    }

    parse_continue :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        location := consume(.KwContinue).location

        // @todo: parse optional label

        // location.end_line = body.location.end_line
        return allocate(AstContinue(
            id          = id
            location    = location
            label       = null
        ))
    }

    parse_defer :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()
        location := consume(.KwDefer).location
        sub := parse_expression(allow_comma, false)
        location = location.to(sub.location)
        return allocate(AstDefer(
            id          = id
            location    = location
            sub         = &*sub
        ))
    }

    parse_return :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        location := consume(.KwReturn).location

        value_expr : ^AstNode = null
        if is_expr_token() {
            value_expr = parse_expression(allow_comma, true)
        }

        // location.end_line = body.location.end_line
        return allocate(AstReturn(
            id          = id
            location    = location
            value_expr  = value_expr
        ))
    }

    values_to_params :: (&Self, values: Array[^AstNode]) -> Array[^AstDecl] {
        params := Array[^AstDecl].create()
        for e in &values {
            match *e {
                AstDecl($decl) -> params.add(decl)
                _ -> {
                    decl := allocate(AstDecl(
                        id          = next_id()
                        location    = e.location
                        pattern     = null
                        type_expr   = *e
                        value_expr  = null
                        mutable     = false
                    ))
                    params.add(decl)
                }
            }
        }
        return params
    }

    parse_parenthesized :: (&Self, allow_comma: bool, allow_function: bool) -> ^AstNode {
        beg := consume(.OpenParen).location

        ends_in_comma := false
        values := Array[^AstNode].create()
        loop {
            next := *lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }

            expr := parse_expression(false, true)
            ends_in_comma = false

            if expr != null {
                values.add(expr)
            }

            next = *lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }

            consume(.Comma)
            ends_in_comma = true
            lexer.skip_whitespace()
        }

        end := consume(.ClosingParen).location

        if allow_function {
            if check_token(.KwConst) {
                lexer.next_token()
                lexer.skip_whitespace()
                body := parse_expression(allow_comma, false)
                return allocate(AstPoly(
                    id          = next_id()
                    location    = beg.to(body.location)
                    param_scope = null
                    params      = values_to_params(values)
                    value_expr  = &*body
                ))
            }
            if check_token(.KwStruct) or check_token(.KwEnum) or check_token(.KwTrait) {
                body := parse_expression(allow_comma, false)
                return allocate(AstPoly(
                    id          = next_id()
                    location    = beg.to(body.location)
                    param_scope = null
                    params      = values_to_params(values)
                    value_expr  = &*body
                ))
            }
            // @todo: parse function if return type '->'
            return_type_expr : ^AstNode = null

            if check_token(.Arrow) {
                lexer.next_token()
                lexer.skip_whitespace()
                return_type_expr = try(parse_expression(true, false))
            }

            if return_type_expr != null or check_token(.OpenBrace) {
                // parse as function if followed by {
                body : ^AstNode = null
                if check_token(.OpenBrace) {
                    body = parse_block()
                }

                return allocate(AstFunction(
                    id                  = next_id()
                    location            = beg.to(body.location)
                    param_scope         = null
                    params              = values_to_params(values)
                    return_type_expr    = return_type_expr
                    body                = body
                ))
            }
        }

        if values.count() == 1 and !ends_in_comma {
            return *values[0]
        }

        return allocate(AstTuple(
            id          = next_id()
            location    = beg.to(end)
            values      = values
        ))
    }

    parse_struct :: (&Self) -> ^AstNode {
        nodes := Array[^AstNode].create()

        beg := consume(.KwStruct).location

        trait_expr : ^AstNode = if !check_token(.OpenBrace) then try(parse_expression(false, false)) else null

        lexer.skip_whitespace()
        consume(.OpenBrace)

        loop {
            lexer.skip_whitespace()
            next := lexer.peek_token()
            if next.typ == .ClosingBrace or next.typ == .EOF {
                break
            }

            node := parse_node()
            if node == null then break
            nodes.add(node)
        }

        end := consume(.ClosingBrace).location

        return allocate(AstStruct(
            id          = next_id()
            location    = beg.to(end)
            trait_expr  = trait_expr
            children    = nodes
        ))
    }

    parse_enum :: (&Self) -> ^AstNode {
        nodes := Array[^AstNode].create()

        beg := consume(.KwEnum).location

        lexer.skip_whitespace()
        consume(.OpenBrace)

        loop {
            lexer.skip_whitespace()
            next := lexer.peek_token()
            if next.typ == .ClosingBrace or next.typ == .EOF {
                break
            }

            node := parse_node()
            if node == null then break
            nodes.add(node)
        }

        end := consume(.ClosingBrace).location

        return allocate(AstEnum(
            id          = next_id()
            location    = beg.to(end)
            children    = nodes
        ))
    }

    parse_trait :: (&Self) -> ^AstNode {
        nodes := Array[^AstNode].create()

        beg := consume(.KwTrait).location

        lexer.skip_whitespace()
        consume(.OpenBrace)

        loop {
            lexer.skip_whitespace()
            next := lexer.peek_token()
            if next.typ == .ClosingBrace or next.typ == .EOF {
                break
            }

            node := parse_node()
            if node == null then break
            nodes.add(node)
        }

        end := consume(.ClosingBrace).location

        return allocate(AstTrait(
            id          = next_id()
            location    = beg.to(end)
            children    = nodes
        ))
    }

    parse_impl :: (&Self) -> ^AstNode {
        nodes := Array[^AstNode].create()
        target_trait : ^AstNode = null

        beg := consume(.KwImpl).location

        lexer.skip_whitespace()
        target := try(parse_expression(false, false))

        lexer.skip_whitespace()
        if check_token(.KwFor) {
            lexer.next_token()
            lexer.skip_whitespace()
            target_trait = target
            target = try(parse_expression(false, false))
            lexer.skip_whitespace()
        }

        consume(.OpenBrace)

        loop {
            lexer.skip_whitespace()
            next := lexer.peek_token()
            if next.typ == .ClosingBrace or next.typ == .EOF {
                break
            }

            node := parse_node()
            if node == null then break
            nodes.add(node)
        }

        end := consume(.ClosingBrace).location

        return allocate(AstImpl(
            id           = next_id()
            location     = beg.to(end)
            children     = nodes
            target_expr  = &*target
            trait_expr   = target_trait
        ))
    }

    parse_use :: (&Self, allow_comma: bool, allow_function: bool) -> ^AstNode {
        beg := consume(.KwUse).location
        value := try(parse_expression(allow_comma, allow_function))
        return allocate(AstUse(
            id          = next_id()
            location    = beg.to(value.location)
            value_expr  = &*value
        ))
    }

    parse_import :: (&Self, allow_comma: bool, allow_function: bool) -> ^AstNode {
        beg := consume(.KwImport).location
        path := try(parse_expression(allow_comma, allow_function))
        return allocate(AstImport(
            id          = next_id()
            location    = beg.to(path.location)
            path        = &*path
        ))
    }

    parse_atomic_expression :: (&Self, allow_comma: bool, allow_function: bool) -> ^AstNode {
        token := lexer.peek_token()
        return match token.typ {
            .Identifier     -> cast(^AstNode) parse_identifier()
            .KwBreak        -> parse_break(allow_comma)
            .KwContinue     -> parse_continue(allow_comma)
            .KwDefer        -> parse_defer(allow_comma)
            .KwEnum         -> parse_enum()
            .KwFalse        -> cast(^AstNode) parse_bool()
            .KwFor          -> parse_for(allow_comma)
            .KwIf           -> parse_if(allow_comma)
            .KwImpl         -> parse_impl()
            .KwImport       -> parse_import(allow_comma, allow_function)
            .KwLoop         -> parse_loop(allow_comma)
            .KwMatch        -> parse_match(allow_comma)
            .KwReturn       -> parse_return(allow_comma)
            .KwStruct       -> parse_struct()
            .KwTrait        -> parse_trait()
            .KwTrue         -> cast(^AstNode) parse_bool()
            .KwUse          -> parse_use(allow_comma, allow_function)
            .NumberLiteral  -> parse_number()
            .OpenBrace      -> parse_block()
            .OpenBracket    -> parse_bracketed()
            .OpenParen      -> parse_parenthesized(allow_comma, allow_function)
            .Period         -> parse_anonymous_dot()
            .StringLiteral  -> cast(^AstNode) parse_string()

            _ -> {
                error_handler.report_error_at(token.location, "Unexpected token {} in expression", [token.typ])
                cast(^AstNode) null
            }
        }
    }

    parse_anonymous_dot :: (&Self) -> ^AstNode {
        beg := consume(.Period).location
        name := try(parse_identifier())
        return allocate(AstDot(
            id              = next_id()
            location        = beg.to(name.location)
            sub             = null
            name            = &*name
        ))
    }

    parse_string :: (&Self) -> ^AstString {
        id := next_id()
        tok := lexer.next_token()
        value := if tok.typ == .StringLiteral {
            tok.data.String
        } else {
            @assert(false)
            return null
        }
        return allocate(AstString(
            id              = id
            location        = tok.location
            string_value    = value
        ))
    }

    parse_bool :: (&Self) -> ^AstBool {
        id := next_id()
        tok := lexer.next_token()
        value := if tok.typ == .KwTrue {
            true
        } else if tok.typ == .KwFalse {
            false
        } else {
            @assert(false)
            return null
        }
        return allocate(AstBool(
            id          = id
            location    = tok.location
            bool_value  = value
        ))
    }

    parse_identifier :: (&Self) -> ^AstIdentifier {
        id := next_id()
        tok := lexer.next_token()
        if tok.typ != .Identifier {
            return null
        }
        name := tok.data.String
        return allocate(AstIdentifier(
            id          = id
            location    = tok.location
            name        = name
        ))
    }

    parse_number :: (&Self) -> ^AstNode {
        id := next_id()
        tok := lexer.next_token()
        if tok.typ != .NumberLiteral {
            return null
        }
        value := match tok.data {
            .Integer($i) -> Value.Int(i)
            .Double($f) -> Value.Float(f)
            _ -> @assert(false)
        }
        return allocate(AstNumberLiteral(
            id          = id
            location    = tok.location
            value       = value
        ))
    }

    parse_argument :: (&Self) -> ^AstArgument {
        id := next_id()

        name : ^AstIdentifier = null
        expr := try(parse_expression(false, true))

        if check_token(.Equal) {
            match &*expr {
                AstIdentifier($id) -> {
                    name = ^*id
                }

                _ -> {
                    error_handler.report_error_at(expr.location, "Name of argument must be an identifier")
                }
            }

            lexer.next_token()
            lexer.skip_whitespace()
            expr = try(parse_expression(false, true))
        }

        return allocate(AstArgument(
            id              = id
            location        = expr.location
            name            = name
            value_expr      = &*expr
        ))
    }

    parse_argument_list :: (&Self, open: TokenType, close: TokenType) -> Array[^AstArgument], Location, Location {
        args := Array[^AstArgument].create()

        beg := consume(open).location
        lexer.skip_whitespace()

        loop {
            next := *lexer.peek_token()
            if next.typ == close or next.typ == .EOF {
                break
            }

            arg := parse_argument()
            if arg == null {
                return args, beg, beg
            }
            args.add(arg)

            next = *lexer.peek_token()
            if next.typ == close or next.typ == .EOF {
                break
            } else if next.typ == .NewLine {
                lexer.skip_whitespace()
            } else {
                consume(.Comma)
                lexer.skip_whitespace()
            }
        }

        end := consume(close).location

        return args, beg, end
    }

    // helpers
    is_expr_token :: (&Self) -> bool {
        token := lexer.peek_token()
        return match token.typ {
            .Identifier         -> true
            .DollarIdentifier   -> true
            .AtSignIdentifier   -> true
            .HashIdentifier     -> true
            .NumberLiteral      -> true
            .StringLiteral      -> true
            .KwTrue             -> true
            .KwFalse            -> true
            .KwIf               -> true
            .KwLoop             -> true
            .KwMatch            -> true
            .KwNull             -> true
            .OpenParen          -> true
            .OpenBrace          -> true
            .OpenBracket        -> true
            .Ampersand          -> true
            .Minus              -> true
            .Asterisk           -> true
            .LessLess           -> true
            .Hat                -> true
            .Bang               -> true
            .Period             -> true

            _                   -> false
        }
    }

    check_token :: (&Self, typ: TokenType) -> bool {
        token := lexer.peek_token()
        return token.typ == typ
    }

    consume :: (&Self, typ: TokenType) -> Token {
        tok := lexer.peek_token()
        while tok.typ != typ {
            match tok.typ {
                TokenType.EOF -> {
                    break
                }
            }

            error_handler.report_error_at(tok.location, "Unexpected token {}, expected {}", [tok.typ, typ])
            lexer.skip_line()
            return *tok
        }

        return lexer.next_token()
    }
}
