use import ast
use import lexer
use import error_handler

use import std.array

mem :: import std.mem.allocator
io  :: import std.io

#export_scope

Parser :: struct {
    lexer           : ref Lexer
    error_handler   : &ErrorHandler
    allocator       : &mem.Allocator

    _next_id        := 1
}

impl Parser {
    new :: (lexer: ref Lexer, error_handler: &ErrorHandler, allocator: &mem.Allocator) -> Self {
        return Parser(
            lexer           = lexer
            error_handler   = error_handler
            allocator       = allocator
        )
    }

    next_id :: (ref Self) -> int {
        return _next_id <- _next_id + 1
    }

    allocate :: (ref Self, value: $T) -> &T {
        result := mem.alloc(T, allocator)
        <<result = value
        return result
    }

    try :: (code: Code) #macro {
        value := @insert(code)
        if value == null {
            return null
        }
        value
    }

    parse_constant_declaration :: (ref Self) -> &AstNode {
        id := next_id()

        lexer.skip_whitespace()
        name := try(self.parse_identifier())

        self.consume(TokenType.Colon)
        self.consume(TokenType.Colon)

        value := try(self.parse_expression())

        location := name.location
        location.end_line = value.location.end_line

        return allocate(AstConstDecl(
            id          = id
            location    = location
            name        = <<name
            type_expr   = null
            value_expr  = <<value
        ))
    }

    parse_expression :: (ref Self) -> &AstNode {
        return parse_post_unary()
    }

    parse_post_unary :: (ref Self) -> &AstNode {
        expr := try(parse_atomic_expression())

        loop {
            match lexer.peek_token().typ {
                .OpenParen -> {
                    id := next_id()
                    args, beg, end := parse_argument_list()
                    location := expr.location
                    location.end_line = end.end_line
                    expr = allocate(AstCall(
                        id          = id
                        location    = location
                        function    = <<expr
                        arguments   = args
                    ))
                }

                _ -> {
                    return expr
                }
            }
        }

        @assert(false)
        return null
    }

    parse_block :: (ref Self) -> &AstNode {
        id := next_id()

        children := Array[&AstNode].create()

        location := consume(.OpenBrace).location
        lexer.skip_whitespace()

        loop {
            next := lexer.peek_token()
            if next.typ == .ClosingBrace or next.typ == .EOF {
                break
            }

            child := try(parse_expression())
            children.add(child)

            next = lexer.peek_token()
            if next.typ == .ClosingBrace or next.typ == .EOF {
                break
            }
            consume(.NewLine)
        }

        lexer.skip_whitespace()
        end := consume(.ClosingBrace).location

        location.end_line = end.end_line
        
        return allocate(AstBlock(
            id          = id
            location    = location
            sub_scope   = null
            children    = children
        ))
    }

    parse_function :: (ref Self) -> &AstNode {
        id := next_id()

        params, location, _ := parse_parameter_list()

        body := try(parse_block())

        location.end_line = body.location.end_line

        return allocate(AstFunction(
            id          = id
            location    = location
            param_scope = null
            params      = params
            body        = body
        ))
    }

    parse_atomic_expression :: (ref Self) -> &AstNode {
        token := lexer.peek_token()
        return match token.typ {
            .Identifier    -> parse_identifier()
            .NumberLiteral -> parse_number()
            .OpenParen     -> parse_function()

            _ -> {
                error_handler.report_error_at(token.location, "Unexpected token {} in expression", [token])
                null
            }
        }
    }

    parse_identifier :: (ref Self) -> &AstIdentifier {
        id := next_id()
        tok := lexer.next_token()
        if tok.typ != .Identifier {
            return null
        }
        name := tok.data.String
        return allocate(AstIdentifier(
            id          = id
            location    = tok.location
            name        = name
        ))
    }

    parse_number :: (ref Self) -> &AstNode {
        id := next_id()
        tok := lexer.next_token()
        if tok.typ != .NumberLiteral {
            return null
        }
        value := tok.data.Integer
        return allocate(AstNumberLiteral(
            id          = id
            location    = tok.location
            int_value   = value
        ))
    }

    parse_parameter :: (ref Self) -> &AstParameter {
        id := next_id()

        name            : &AstIdentifier = null
        type_expr       : &AstNode       = null
        default_vaule   : &AstNode       = null

        expr := try(parse_expression())
        location := expr.location

        if check_token(.Colon) {
            match ref <<expr {
                AstIdentifier($id) -> {
                    name = &id
                }
                _ -> {
                    error_handler.report_error_at(expr.location, "Name of parameter must be an identifier", [])
                    return null
                }
            }

            consume(.Colon)
            lexer.skip_whitespace()
            type_expr = parse_expression()
        } else {
            type_expr = expr
        }

        end := type_expr.location

        if name == null {
            error_handler.report_error_at(location, "Parameter has no name", [])
            return null
        }

        return allocate(AstParameter(
            id              = id
            location        = location.to(end)
            name            = <<name
            type_expr       = type_expr
            default_value   = default_vaule
        ))
    }

    parse_parameter_list :: (ref Self) -> Array[&AstParameter], Location, Location {
        params := Array[&AstParameter].create()

        beg := consume(.OpenParen).location
        lexer.skip_whitespace()

        loop {
            next := lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }

            param := parse_parameter()

            if param != null{
                params.add(param)
            }

            next = lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }

            consume(.Comma)
            lexer.skip_whitespace()
        }

        end := consume(.ClosingParen).location

        return params, beg, end
    }

    parse_argument_list :: (ref Self) -> Array[&AstNode], Location, Location {
        args := Array[&AstNode].create()

        beg := consume(.OpenParen).location
        lexer.skip_whitespace()

        loop {
            next := lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }

            arg := parse_expression()
            if arg == null {
                return args, beg, beg
            }
            args.add(arg)
            lexer.skip_whitespace()

            next = lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }
            
            consume(.Comma)
            lexer.skip_whitespace()
        }

        end := consume(.ClosingParen).location

        return args, beg, end
    }

    // helpers
    check_token :: (ref Self, typ: TokenType) -> bool {
        token := lexer.peek_token()
        return token.typ == typ
    }

    consume :: (ref Self, typ: TokenType) -> Token {
        tok := lexer.peek_token()
        while tok.typ != typ {
            match tok.typ {
                TokenType.EOF -> {
                    break
                }
            }

            error_handler.report_error_at(tok.location, "Unexpeted token {}, expected {}", [tok, typ])
            lexer.skip_line()
            return tok
        }

        return lexer.next_token()
    }
}
