use import ast
use import lexer
use import error_handler

use import std.array

mem :: import std.mem.allocator
io  :: import std.io

#export_scope

Parser :: struct {
    lexer           : &Lexer
    error_handler   : ^ErrorHandler
    allocator       : ^mem.Allocator

    _next_id        := 1
}

impl Parser {
    new :: (lexer: &Lexer, error_handler: ^ErrorHandler, allocator: ^mem.Allocator) -> Self {
        return Parser(
            lexer           = lexer
            error_handler   = error_handler
            allocator       = allocator
        )
    }

    next_id :: (&Self) -> int {
        return _next_id <- _next_id + 1
    }

    allocate :: (&Self, value: $T) -> ^T {
        result := mem.alloc(T, allocator)
        <<result = value
        return result
    }

    try :: (code: Code) #macro {
        value := @insert(code)
        if value == null {
            return null
        }
        value
    }

    parse_node :: (&Self) -> ^AstNode {
        if check_token(.EOF) {
            return null
        }
        expr := parse_expression(true)
        consume(.NewLine)
        return expr
    }

    parse_expression :: (&Self, allow_comma: bool) -> ^AstNode {
        expr := parse_comma_expression(allow_comma)

        if check_token(.Equal) {
            lexer.next_token()
            lexer.skip_whitespace()
            value_expr := parse_comma_expression(allow_comma)

            return allocate(AstAssignment(
                id          = next_id()
                location    = expr.location.to(value_expr.location)
                pattern     = <<expr
                value_expr  = <<value_expr
            ))
        } else if check_token(.Colon) {
            lexer.next_token()
            lexer.skip_whitespace()

            type_expr : ^AstNode = if check_token(.Equal) or check_token(.Colon) {
                null
            } else {
                parse_comma_expression(allow_comma)
            }

            if check_token(.Colon) {
                lexer.next_token()
                lexer.skip_whitespace()
                value_expr := parse_comma_expression(allow_comma)

                return allocate(AstConstDecl(
                    id          = next_id()
                    location    = expr.location.to(value_expr.location)
                    pattern     = <<expr
                    type_expr   = type_expr
                    value_expr  = <<value_expr
                ))
            } else if check_token(.Equal) {
                lexer.next_token()
                lexer.skip_whitespace()
                value_expr := parse_comma_expression(allow_comma)

                return allocate(AstDecl(
                    id          = next_id()
                    location    = expr.location.to(value_expr.location)
                    pattern     = <<expr
                    type_expr   = type_expr
                    value_expr  = value_expr
                ))
            } else {
                return allocate(AstDecl(
                    id          = next_id()
                    location    = expr.location.to(type_expr.location)
                    pattern     = <<expr
                    type_expr   = type_expr
                    value_expr  = null
                ))
            }
        }
        
        return expr
    }

    parse_comma_expression :: (&Self, allow_comma: bool) -> ^AstNode {
        expr := parse_binary(allow_comma)

        if allow_comma and check_token(.Comma) {
            values := Array[^AstNode].create()
            values.add(expr)
            location := expr.location

            while check_token(.Comma) {
                lexer.next_token()
                lexer.skip_whitespace()
                values.add(parse_binary(true))
                location = location.to(values.peek_last().location)
            }

            id := next_id()
            expr = allocate(AstTuple(
                id          = id
                location    = location
                values      = values 
            ))
        }

        return expr
    }

    parse_binary :: (&Self, allow_comma: bool, precedence: int = int.min) -> ^AstNode {
        expr := parse_unary(allow_comma)

        loop {
            next := lexer.peek_token()

            op, pre := match next.typ {
                .ReverseArrow   -> AstBinary.BinOp.Move,         2

                .Pipe           -> AstBinary.BinOp.Pipe,         3

                .KwAnd          -> AstBinary.BinOp.And,          4
                .KwOr           -> AstBinary.BinOp.Or,           4

                .Less           -> AstBinary.BinOp.Less,         5
                .LessEqual      -> AstBinary.BinOp.LessEq,       5
                .Greater        -> AstBinary.BinOp.Greater,      5
                .GreaterEqual   -> AstBinary.BinOp.GreaterEq,    5
                .DoubleEqual    -> AstBinary.BinOp.Equal,        5
                .NotEqual       -> AstBinary.BinOp.NotEqual,     5

                .PeriodPeriod   -> AstBinary.BinOp.Range,        6
                .PeriodPeriodEq -> AstBinary.BinOp.RangeIncl,    6

                .Plus           -> AstBinary.BinOp.Add,          7
                .Minus          -> AstBinary.BinOp.Sub,          7

                .Asterisk       -> AstBinary.BinOp.Mul,          8
                .ForwardSlash   -> AstBinary.BinOp.Div,          8
                .Percent        -> AstBinary.BinOp.Mod,          8

                _ -> {
                    return expr
                }
            }

            if pre <= precedence {
                return expr
            }

            lexer.next_token()
            lexer.skip_whitespace()

            id := next_id()
            right := parse_binary(allow_comma, pre)
            location := expr.location.to(right.location)
            expr = allocate(AstBinary(
                id          = id
                location    = location
                operator    = op
                left        = <<expr
                right       = <<right
            ))
        }

        @assert(false)
        return null
    }

    parse_unary :: (&Self, allow_comma: bool) -> ^AstNode {
        return parse_post_unary(allow_comma)
    }

    parse_post_unary :: (&Self, allow_comma: bool) -> ^AstNode {
        expr := try(parse_atomic_expression(allow_comma))

        loop {
            match lexer.peek_token().typ {
                .OpenParen -> {
                    id := next_id()
                    args, beg, end := parse_argument_list(allow_comma)
                    location := expr.location
                    location.end_line = end.end_line
                    expr = allocate(AstCall(
                        id          = id
                        location    = location
                        function    = <<expr
                        arguments   = args
                    ))
                }

                _ -> {
                    return expr
                }
            }
        }

        @assert(false)
        return null
    }

    parse_block :: (&Self) -> ^AstNode {
        id := next_id()

        children := Array[^AstNode].create()

        location := consume(.OpenBrace).location
        lexer.skip_whitespace()

        loop {
            next := lexer.peek_token()
            if next.typ == .ClosingBrace or next.typ == .EOF {
                break
            }

            child := try(parse_expression(true))
            children.add(child)

            next = lexer.peek_token()
            if next.typ == .ClosingBrace or next.typ == .EOF {
                break
            }
            consume(.NewLine)
        }

        lexer.skip_whitespace()
        end := consume(.ClosingBrace).location

        location.end_line = end.end_line
        
        return allocate(AstBlock(
            id          = id
            location    = location
            sub_scope   = null
            children    = children
        ))
    }

    parse_function :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        params, location, _ := parse_parameter_list(allow_comma)

        body := try(parse_block())

        location.end_line = body.location.end_line

        return allocate(AstFunction(
            id          = id
            location    = location
            param_scope = null
            params      = params
            body        = body
        ))
    }

    parse_if :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        location := consume(.KwIf).location

        lexer.skip_whitespace()
        condition := parse_expression(false)

        true_case := if check_token(.KwThen) {
            lexer.next_token()
            parse_expression(allow_comma)
        } else {
            parse_block()
        }

        location = location.to(true_case.location)

        false_case : ^AstNode = if check_token(.KwElse) {
            lexer.next_token()
            expr := parse_expression(allow_comma)
            location = location.to(expr.location)
            expr
        } else {
            null
        }

        return allocate(AstIf(
            id          = id
            location    = location
            condition   = <<condition
            true_case   = <<true_case
            false_case  = false_case
        ))
    }

    parse_loop :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()
        location := consume(.KwLoop).location
        body := parse_expression(allow_comma)
        location = location.to(body.location)
        return allocate(AstLoop(
            id          = id
            location    = location
            body        = <<body
        ))
    }

    parse_for :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        location := consume(.KwFor).location

        collection := parse_expression(false)
        body := parse_block()

        location = location.to(body.location)
        return allocate(AstFor(
            id          = id
            location    = location
            it_name     = null
            index_name  = null
            collection  = <<collection
            body        = <<body
        ))
    }

    parse_break :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        location := consume(.KwBreak).location

        // location.end_line = body.location.end_line
        return allocate(AstBreak(
            id          = id
            location    = location
            label       = null
            value_expr  = null
        ))
    }

    parse_continue :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        location := consume(.KwContinue).location

        // location.end_line = body.location.end_line
        return allocate(AstContinue(
            id          = id
            location    = location
            label       = null
        ))
    }

    parse_defer :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()
        location := consume(.KwDefer).location
        sub := parse_expression(allow_comma)
        location = location.to(sub.location)
        return allocate(AstDefer(
            id          = id
            location    = location
            sub         = <<sub
        ))
    }

    parse_return :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        location := consume(.KwReturn).location

        // location.end_line = body.location.end_line
        return allocate(AstReturn(
            id          = id
            location    = location
            value_expr  = null
        ))
    }

    parse_tuple :: (&Self) -> ^AstNode {
        beg := consume(.OpenParen).location

        values := Array[^AstNode].create()
        loop {
            next := lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }

            expr := parse_expression(false)

            if expr != null {
                values.add(expr)
            }

            next = lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }

            consume(.Comma)
            lexer.skip_whitespace()
        }

        end := consume(.ClosingParen).location

        // @todo: parse function here?

        return allocate(AstTuple(
            id          = next_id()
            location    = beg.to(end)
            values      = values
        ))
    }

    parse_atomic_expression :: (&Self, allow_comma: bool) -> ^AstNode {
        token := lexer.peek_token()
        return match token.typ {
            .OpenBrace      -> parse_block()
            .Identifier     -> parse_identifier()
            .NumberLiteral  -> parse_number()
            .KwTrue         -> parse_bool()
            .KwFalse        -> parse_bool()
            .StringLiteral  -> parse_string()
            .OpenParen      -> parse_tuple()
            .KwIf           -> parse_if(allow_comma)
            .KwLoop         -> parse_loop(allow_comma)
            .KwFor          -> parse_for(allow_comma)
            .KwBreak        -> parse_break(allow_comma)
            .KwContinue     -> parse_continue(allow_comma)
            .KwDefer        -> parse_defer(allow_comma)
            .KwReturn       -> parse_return(allow_comma)

            _ -> {
                error_handler.report_error_at(token.location, "Unexpected token {} in expression", [token])
                null
            }
        }
    }

    parse_string :: (&Self) -> ^AstString {
        id := next_id()
        tok := lexer.next_token()
        value := if tok.typ == .StringLiteral {
            tok.data.String
        } else {
            @assert(false)
            return null
        }
        return allocate(AstString(
            id              = id
            location        = tok.location
            string_value    = value
        ))
    }

    parse_bool :: (&Self) -> ^AstBool {
        id := next_id()
        tok := lexer.next_token()
        value := if tok.typ == .KwTrue {
            true
        } else if tok.typ == .KwFalse {
            false
        } else {
            @assert(false)
            return null
        }
        return allocate(AstBool(
            id          = id
            location    = tok.location
            bool_value  = value
        ))
    }

    parse_identifier :: (&Self) -> ^AstIdentifier {
        id := next_id()
        tok := lexer.next_token()
        if tok.typ != .Identifier {
            return null
        }
        name := tok.data.String
        return allocate(AstIdentifier(
            id          = id
            location    = tok.location
            name        = name
        ))
    }

    parse_number :: (&Self) -> ^AstNode {
        id := next_id()
        tok := lexer.next_token()
        if tok.typ != .NumberLiteral {
            return null
        }
        value := tok.data.Integer
        return allocate(AstNumberLiteral(
            id          = id
            location    = tok.location
            int_value   = value
        ))
    }

    parse_parameter :: (&Self, allow_comma: bool) -> ^AstParameter {
        id := next_id()

        name            : ^AstIdentifier = null
        type_expr       : ^AstNode       = null
        default_vaule   : ^AstNode       = null

        expr := try(parse_expression(allow_comma))
        location := expr.location

        if check_token(.Colon) {
            match &<<expr {
                AstIdentifier($id) -> {
                    name = ^id
                }
                _ -> {
                    error_handler.report_error_at(expr.location, "Name of parameter must be an identifier", [])
                    return null
                }
            }

            consume(.Colon)
            lexer.skip_whitespace()
            type_expr = parse_expression(allow_comma)
        } else {
            type_expr = expr
        }

        end := type_expr.location

        if name == null {
            error_handler.report_error_at(location, "Parameter has no name", [])
            return null
        }

        return allocate(AstParameter(
            id              = id
            location        = location.to(end)
            name            = <<name
            type_expr       = type_expr
            default_value   = default_vaule
        ))
    }

    parse_argument :: (&Self, allow_comma: bool) -> ^AstArgument {
        id := next_id()

        name : ^AstIdentifier = null
        expr := try(parse_expression(allow_comma))

        return allocate(AstArgument(
            id              = id
            location        = expr.location
            name            = name
            value_expr      = <<expr
        ))
    }

    parse_parameter_list :: (&Self, allow_comma: bool) -> Array[^AstParameter], Location, Location {
        params := Array[^AstParameter].create()

        beg := consume(.OpenParen).location
        lexer.skip_whitespace()

        loop {
            next := lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }

            param := parse_parameter(allow_comma)

            if param != null{
                params.add(param)
            }

            next = lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }

            consume(.Comma)
            lexer.skip_whitespace()
        }

        end := consume(.ClosingParen).location

        return params, beg, end
    }

    parse_argument_list :: (&Self, allow_comma: bool) -> Array[^AstArgument], Location, Location {
        args := Array[^AstArgument].create()

        beg := consume(.OpenParen).location
        lexer.skip_whitespace()

        loop {
            next := lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }

            arg := parse_argument(allow_comma)
            if arg == null {
                return args, beg, beg
            }
            args.add(arg)
            lexer.skip_whitespace()

            next = lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }
            
            consume(.Comma)
            lexer.skip_whitespace()
        }

        end := consume(.ClosingParen).location

        return args, beg, end
    }

    // helpers
    check_token :: (&Self, typ: TokenType) -> bool {
        token := lexer.peek_token()
        return token.typ == typ
    }

    consume :: (&Self, typ: TokenType) -> Token {
        tok := lexer.peek_token()
        while tok.typ != typ {
            match tok.typ {
                TokenType.EOF -> {
                    break
                }
            }

            error_handler.report_error_at(tok.location, "Unexpeted token {}, expected {}", [tok, typ])
            lexer.skip_line()
            return tok
        }

        return lexer.next_token()
    }
}
