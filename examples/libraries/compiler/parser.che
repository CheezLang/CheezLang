use import std.array
use import std.fiber

mem :: import std.mem.allocator
io  :: import std.io
fmt :: import std.fmt

use import logging.logger

use import ast
use import compiler
use import config
use import error_handler
use import lexer
use import value

#export_scope

Parser :: struct {
    lexer           : &Lexer
    error_handler   : ^ErrorHandler
    allocator       : ^mem.Allocator

    _next_id        := 1
}

impl Parser {
    debug_log :: (&Self, msg: string, args: []^any = []) {
        if const DEBUG_PARSER {
            str := fmt.format(msg, args)
            location := lexer.current_location()
            g_logger.log("[{}] [Parser] {}:{}:{}: {}", [Fiber.user_data(FiberContext).thread, location.file, location.line, location.column, str])
        } 
    }

    new :: (lexer: &Lexer, error_handler: ^ErrorHandler, allocator: ^mem.Allocator, first_id: int) -> Self {
        return Parser(
            lexer           = lexer
            error_handler   = error_handler
            allocator       = allocator
            _next_id        = first_id
        )
    }

    next_id :: (&Self) -> int {
        return _next_id <- _next_id + 1
    }

    allocate :: (&Self, value: $T) -> ^T {
        result := mem.alloc(T, allocator)
        <<result = value
        return result
    }

    try :: (code: Code) #macro {
        value := @insert(code)
        if value == null {
            return null
        }
        value
    }

    parse_node :: (&Self) -> ^AstNode {
        lexer.skip_whitespace()
        if check_token(.EOF) {
            return null
        }
        expr := parse_expression(true, true)
        consume(.NewLine)
        return expr
    }

    parse_expression :: (&Self, allow_comma: bool, allow_function: bool) -> ^AstNode {
        if check_token(.KwMut) {
            return parse_declaration(allow_comma, allow_function)
        }

        expr := parse_comma_expression(allow_comma, allow_function)

        if check_token(.Equal) {
            lexer.next_token()
            lexer.skip_whitespace()
            value_expr := parse_comma_expression(allow_comma, allow_function)

            return allocate(AstAssignment(
                id          = next_id()
                location    = expr.location.to(value_expr.location)
                pattern     = <<expr
                value_expr  = <<value_expr
            ))
        } else if check_token(.Colon) {
            lexer.next_token()
            lexer.skip_whitespace()

            type_expr : ^AstNode = if check_token(.Equal) or check_token(.Colon) {
                null
            } else {
                parse_comma_expression(allow_comma, allow_function)
            }

            if check_token(.Colon) {
                lexer.next_token()
                lexer.skip_whitespace()
                value_expr := parse_comma_expression(allow_comma, allow_function)

                return allocate(AstConstDecl(
                    id          = next_id()
                    location    = expr.location.to(value_expr.location)
                    pattern     = <<expr
                    type_expr   = type_expr
                    value_expr  = <<value_expr
                ))
            } else if check_token(.Equal) {
                lexer.next_token()
                lexer.skip_whitespace()
                value_expr := parse_comma_expression(allow_comma, allow_function)

                return allocate(AstDecl(
                    id          = next_id()
                    location    = expr.location.to(value_expr.location)
                    pattern     = expr
                    type_expr   = type_expr
                    value_expr  = value_expr
                    mutable     = false
                ))
            } else {
                return allocate(AstDecl(
                    id          = next_id()
                    location    = expr.location.to(type_expr.location)
                    pattern     = expr
                    type_expr   = type_expr
                    value_expr  = null
                    mutable     = false
                ))
            }
        }
        
        return expr
    }

    parse_declaration :: (&Self, allow_comma: bool, allow_function: bool) -> ^AstNode {
        mutable := false
        if check_token(.KwMut) {
            lexer.next_token()
            mutable = true
        }

        pattern := parse_comma_expression(allow_comma, allow_function)

        consume(.Colon)
        lexer.skip_whitespace()

        type_expr : ^AstNode = if check_token(.Equal) or check_token(.Colon) {
            null
        } else {
            parse_comma_expression(allow_comma, allow_function)
        }

        if check_token(.Equal) {
            lexer.next_token()
            lexer.skip_whitespace()
            value_expr := parse_comma_expression(allow_comma, allow_function)

            return allocate(AstDecl(
                id          = next_id()
                location    = pattern.location.to(value_expr.location)
                pattern     = pattern
                type_expr   = type_expr
                value_expr  = value_expr
                mutable     = mutable
            ))
        } else {
            return allocate(AstDecl(
                id          = next_id()
                location    = pattern.location.to(type_expr.location)
                pattern     = pattern
                type_expr   = type_expr
                value_expr  = null
                mutable     = mutable
            ))
        }
        
        return pattern
    }

    parse_comma_expression :: (&Self, allow_comma: bool, allow_function: bool) -> ^AstNode {
        expr := parse_binary(allow_comma, allow_function)

        if allow_comma and check_token(.Comma) {
            values := Array[^AstNode].create()
            values.add(expr)
            location := expr.location

            while check_token(.Comma) {
                lexer.next_token()
                lexer.skip_whitespace()
                values.add(parse_binary(true, allow_function))
                location = location.to(values.peek_last().location)
            }

            id := next_id()
            expr = allocate(AstTuple(
                id          = id
                location    = location
                values      = values 
            ))
        }

        return expr
    }

    parse_binary :: (&Self, allow_comma: bool, allow_function: bool, precedence: int = int.min, left_associative: bool = true) -> ^AstNode {
        expr := parse_unary(allow_comma, allow_function)

        loop {
            next := lexer.peek_token()

            op, pre, left_asso := match next.typ {
                .ReverseArrow   -> AstBinary.BinOp.Move,         3, false
                .Pipe           -> AstBinary.BinOp.Pipe,         3, true

                .KwAnd          -> AstBinary.BinOp.And,          4, true
                .KwOr           -> AstBinary.BinOp.Or,           4, true

                .Less           -> AstBinary.BinOp.Less,         5, true
                .LessEqual      -> AstBinary.BinOp.LessEq,       5, true
                .Greater        -> AstBinary.BinOp.Greater,      5, true
                .GreaterEqual   -> AstBinary.BinOp.GreaterEq,    5, true
                .DoubleEqual    -> AstBinary.BinOp.Equal,        5, true
                .NotEqual       -> AstBinary.BinOp.NotEqual,     5, true

                .PeriodPeriod   -> AstBinary.BinOp.Range,        6, true
                .PeriodPeriodEq -> AstBinary.BinOp.RangeIncl,    6, true

                .Plus           -> AstBinary.BinOp.Add,          7, true
                .Minus          -> AstBinary.BinOp.Sub,          7, true

                .Asterisk       -> AstBinary.BinOp.Mul,          8, true
                .ForwardSlash   -> AstBinary.BinOp.Div,          8, true
                .Percent        -> AstBinary.BinOp.Mod,          8, true

                _ -> {
                    return expr
                }
            }

            if pre == precedence and left_asso != left_associative {
                error_handler.report_error_at(next.location, "Ambigious operator precedence. Use parenthesis to disambiguate")
                return expr
            }

            if (left_asso and pre <= precedence) or (!left_asso and pre < precedence) {
                return expr
            }

            lexer.next_token()
            lexer.skip_whitespace()

            id := next_id()
            right := parse_binary(allow_comma, allow_function, pre, left_asso)
            location := expr.location.to(right.location)
            expr = allocate(AstBinary(
                id          = id
                location    = location
                operator    = op
                left        = <<expr
                right       = <<right
            ))
        }

        @assert(false)
        return null
    }

    parse_unary :: (&Self, allow_comma: bool, allow_function: bool) -> ^AstNode {
        next := lexer.peek_token()
        match next.typ {
            .Minus -> {
                beg := lexer.next_token().location
                sub := parse_unary(allow_comma, allow_function)
                return allocate(AstUnary(
                    id          = next_id()
                    location    = beg.to(sub.location)
                    operator    = .Neg
                    sub         = <<sub
                ))
            }
            .Bang -> {
                debug_log("unary bang {}", [lexer.peek_token().typ])
                beg := lexer.next_token().location
                sub := parse_unary(allow_comma, allow_function)
                return allocate(AstUnary(
                    id          = next_id()
                    location    = beg.to(sub.location)
                    operator    = .Not
                    sub         = <<sub
                ))
            }
            .Ampersand -> {
                beg := lexer.next_token().location

                mutable := if check_token(.KwMut) {
                    lexer.next_token()
                    true
                } else {
                    false
                }

                sub := parse_unary(allow_comma, allow_function)
                return allocate(AstUnary(
                    id          = next_id()
                    location    = beg.to(sub.location)
                    operator    = if mutable then .RefMut else .Ref
                    sub         = <<sub
                ))
            }
            .Hat -> {
                beg := lexer.next_token().location
                sub := parse_unary(allow_comma, allow_function)
                return allocate(AstUnary(
                    id          = next_id()
                    location    = beg.to(sub.location)
                    operator    = .Ptr
                    sub         = <<sub
                ))
            }
        }
        return parse_post_unary(allow_comma, allow_function)
    }

    parse_post_unary :: (&Self, allow_comma: bool, allow_function: bool) -> ^AstNode {
        debug_log("parse_post_unary")
        expr := try(parse_atomic_expression(allow_comma, allow_function))

        loop {
            match lexer.peek_token().typ {
                .OpenParen -> {
                    id := next_id()
                    args, beg, end := parse_argument_list()
                    expr = allocate(AstCall(
                        id          = id
                        location    = expr.location.to(end)
                        function    = <<expr
                        arguments   = args
                    ))
                }

                .Period -> {
                    lexer.next_token()
                    name := try(parse_identifier())
                    expr = allocate(AstDot(
                        id          = next_id()
                        location    = expr.location.to(name.location)
                        sub         = expr
                        name        = <<name
                    ))
                }

                _ -> {
                    debug_log("return expr")
                    return expr
                }
            }
        }

        @assert(false)
        return null
    }

    parse_bracketed :: (&Self) -> ^AstNode {
        beg := consume(.OpenBracket).location

        values := Array[^AstNode].create()
        loop {
            next := lexer.peek_token()
            if next.typ == .ClosingBracket or next.typ == .EOF {
                break
            }

            expr := parse_expression(false, true)

            if expr != null {
                values.add(expr)
            }

            next = lexer.peek_token()
            if next.typ == .ClosingBracket or next.typ == .EOF {
                break
            }

            consume(.Comma)
            lexer.skip_whitespace()
        }

        end := consume(.ClosingBracket).location

        if is_expr_token() {
            if values.count() > 1 {
                error_handler.report_error_at(beg.to(end), "Too many values in array type expression")
            }

            target := try(parse_expression(false, false))

            count : ^AstNode = null
            if values.count() > 0 {
                count = values[0]
            }
            return allocate(AstArrayType(
                id          = next_id()
                location    = beg.to(target.location)
                count       = count
                target      = <<target
            ))
        }


        return allocate(AstArray(
            id          = next_id()
            location    = beg.to(end)
            values      = values
        ))
    }


    parse_block :: (&Self) -> ^AstNode {
        id := next_id()

        children := Array[^AstNode].create()

        location := consume(.OpenBrace).location
        lexer.skip_whitespace()

        loop {
            next := lexer.peek_token()
            if next.typ == .ClosingBrace or next.typ == .EOF {
                break
            }

            child := try(parse_expression(true, true))
            children.add(child)

            next = lexer.peek_token()
            if next.typ == .ClosingBrace or next.typ == .EOF {
                break
            }
            consume(.NewLine)
        }

        lexer.skip_whitespace()
        end := consume(.ClosingBrace).location

        location.end_line = end.end_line
        
        return allocate(AstBlock(
            id          = id
            location    = location
            sub_scope   = null
            children    = children
        ))
    }

    parse_if :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        location := consume(.KwIf).location

        lexer.skip_whitespace()
        condition := parse_expression(false, false)

        true_case := if check_token(.KwThen) {
            lexer.next_token()
            parse_expression(allow_comma, false)
        } else {
            parse_block()
        }

        location = location.to(true_case.location)

        false_case : ^AstNode = if check_token(.KwElse) {
            lexer.next_token()
            expr := parse_expression(allow_comma, false)
            location = location.to(expr.location)
            expr
        } else {
            null
        }

        return allocate(AstIf(
            id          = id
            location    = location
            condition   = <<condition
            true_case   = <<true_case
            false_case  = false_case
        ))
    }

    parse_loop :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()
        location := consume(.KwLoop).location
        body := parse_expression(allow_comma, false)
        location = location.to(body.location)
        return allocate(AstLoop(
            id          = id
            location    = location
            body        = <<body
        ))
    }

    parse_for :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        location := consume(.KwFor).location

        // @todo: parse names

        collection := parse_expression(false, false)
        body := parse_block()

        location = location.to(body.location)
        return allocate(AstFor(
            id          = id
            location    = location
            it_name     = null
            index_name  = null
            collection  = <<collection
            body        = <<body
        ))
    }

    parse_break :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        location := consume(.KwBreak).location

        // @todo: parse optional label and expression

        // location.end_line = body.location.end_line
        return allocate(AstBreak(
            id          = id
            location    = location
            label       = null
            value_expr  = null
        ))
    }

    parse_continue :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        location := consume(.KwContinue).location

        // @todo: parse optional label

        // location.end_line = body.location.end_line
        return allocate(AstContinue(
            id          = id
            location    = location
            label       = null
        ))
    }

    parse_defer :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()
        location := consume(.KwDefer).location
        sub := parse_expression(allow_comma, false)
        location = location.to(sub.location)
        return allocate(AstDefer(
            id          = id
            location    = location
            sub         = <<sub
        ))
    }

    parse_return :: (&Self, allow_comma: bool) -> ^AstNode {
        id := next_id()

        location := consume(.KwReturn).location

        value_expr : ^AstNode = null
        if is_expr_token() {
            value_expr = parse_expression(allow_comma, true)
        }

        // location.end_line = body.location.end_line
        return allocate(AstReturn(
            id          = id
            location    = location
            value_expr  = value_expr
        ))
    }

    parse_parenthesized :: (&Self, allow_function: bool) -> ^AstNode {
        beg := consume(.OpenParen).location

        ends_in_comma := false
        values := Array[^AstNode].create()
        loop {
            next := lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }

            expr := parse_expression(false, true)
            ends_in_comma = false

            if expr != null {
                values.add(expr)
            }

            next = lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }

            consume(.Comma)
            ends_in_comma = true
            lexer.skip_whitespace()
        }

        end := consume(.ClosingParen).location

        if allow_function {
            // @todo: parse function if return type '->'
            return_type_expr : ^AstNode = null

            if check_token(.Arrow) {
                lexer.next_token()
                lexer.skip_whitespace()
                return_type_expr = try(parse_expression(true, false))
            }

            if return_type_expr != null or check_token(.OpenBrace) {
                params := Array[^AstDecl].create()
                for e : values {
                    match &<<e {
                        AstDecl($decl) -> params.add(^decl)
                        _ -> {
                            decl := allocate(AstDecl(
                                id          = next_id()
                                location    = e.location
                                pattern     = null
                                type_expr   = e
                                value_expr  = null
                                mutable     = false
                            ))
                            params.add(decl)
                        }
                    }
                }

                // parse as function if followed by {
                body : ^AstNode = null
                if check_token(.OpenBrace) {
                    body = parse_block()
                }

                return allocate(AstFunction(
                    id                  = next_id()
                    location            = beg.to(body.location)
                    param_scope         = null
                    params              = params
                    return_type_expr    = return_type_expr
                    body                = body
                ))
            }
        }

        if values.count() == 1 and !ends_in_comma {
            return values[0]
        }

        return allocate(AstTuple(
            id          = next_id()
            location    = beg.to(end)
            values      = values
        ))
    }

    parse_atomic_expression :: (&Self, allow_comma: bool, allow_function: bool) -> ^AstNode {
        token := lexer.peek_token()
        return match token.typ {
            .OpenParen      -> parse_parenthesized(allow_function)
            .OpenBracket    -> parse_bracketed()
            .OpenBrace      -> parse_block()
            .Identifier     -> parse_identifier()
            .NumberLiteral  -> parse_number()
            .KwTrue         -> parse_bool()
            .KwFalse        -> parse_bool()
            .StringLiteral  -> parse_string()
            .KwIf           -> parse_if(allow_comma)
            .KwLoop         -> parse_loop(allow_comma)
            .KwFor          -> parse_for(allow_comma)
            .KwBreak        -> parse_break(allow_comma)
            .KwContinue     -> parse_continue(allow_comma)
            .KwDefer        -> parse_defer(allow_comma)
            .KwReturn       -> parse_return(allow_comma)

            _ -> {
                error_handler.report_error_at(token.location, "Unexpected token {} in expression", [token])
                null
            }
        }
    }

    parse_string :: (&Self) -> ^AstString {
        id := next_id()
        tok := lexer.next_token()
        value := if tok.typ == .StringLiteral {
            tok.data.String
        } else {
            @assert(false)
            return null
        }
        return allocate(AstString(
            id              = id
            location        = tok.location
            string_value    = value
        ))
    }

    parse_bool :: (&Self) -> ^AstBool {
        id := next_id()
        tok := lexer.next_token()
        value := if tok.typ == .KwTrue {
            true
        } else if tok.typ == .KwFalse {
            false
        } else {
            @assert(false)
            return null
        }
        return allocate(AstBool(
            id          = id
            location    = tok.location
            bool_value  = value
        ))
    }

    parse_identifier :: (&Self) -> ^AstIdentifier {
        id := next_id()
        tok := lexer.next_token()
        if tok.typ != .Identifier {
            return null
        }
        name := tok.data.String
        return allocate(AstIdentifier(
            id          = id
            location    = tok.location
            name        = name
        ))
    }

    parse_number :: (&Self) -> ^AstNode {
        id := next_id()
        tok := lexer.next_token()
        if tok.typ != .NumberLiteral {
            return null
        }
        value := match tok.data {
            .Integer($i) -> Value.Int(i)
            .Double($f) -> Value.Float(f)
            _ -> @assert(false)
        }
        return allocate(AstNumberLiteral(
            id          = id
            location    = tok.location
            value       = value
        ))
    }

    parse_argument :: (&Self) -> ^AstArgument {
        id := next_id()

        name : ^AstIdentifier = null
        expr := try(parse_expression(false, true))

        if check_token(.Equal) {
            match &<<expr {
                AstIdentifier($id) -> {
                    name = ^id
                }

                _ -> {
                    error_handler.report_error_at(expr.location, "Name of argument must be an identifier")
                }
            }

            lexer.next_token()
            lexer.skip_whitespace()
            expr = try(parse_expression(false, true))
        }

        return allocate(AstArgument(
            id              = id
            location        = expr.location
            name            = name
            value_expr      = <<expr
        ))
    }

    parse_argument_list :: (&Self) -> Array[^AstArgument], Location, Location {
        args := Array[^AstArgument].create()

        beg := consume(.OpenParen).location
        lexer.skip_whitespace()

        loop {
            next := lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }

            arg := parse_argument()
            if arg == null {
                return args, beg, beg
            }
            args.add(arg)
            lexer.skip_whitespace()

            next = lexer.peek_token()
            if next.typ == .ClosingParen or next.typ == .EOF {
                break
            }
            
            consume(.Comma)
            lexer.skip_whitespace()
        }

        end := consume(.ClosingParen).location

        return args, beg, end
    }

    // helpers
    is_expr_token :: (&Self) -> bool {
        token := lexer.peek_token()
        return match token.typ {
            .Identifier         -> true
            .DollarIdentifier   -> true
            .AtSignIdentifier   -> true
            .HashIdentifier     -> true
            .NumberLiteral      -> true
            .StringLiteral      -> true
            .KwTrue             -> true
            .KwFalse            -> true
            .KwIf               -> true
            .KwLoop             -> true
            .KwMatch            -> true
            .KwNull             -> true
            .OpenParen          -> true
            .OpenBrace          -> true
            .OpenBracket        -> true
            .Ampersand          -> true
            .Minus              -> true
            .Asterisk           -> true
            .LessLess           -> true
            .Hat                -> true
            .Bang               -> true
            .Period             -> true

            _                   -> false
        }
    }

    check_token :: (&Self, typ: TokenType) -> bool {
        token := lexer.peek_token()
        return token.typ == typ
    }

    consume :: (&Self, typ: TokenType) -> Token {
        tok := lexer.peek_token()
        while tok.typ != typ {
            match tok.typ {
                TokenType.EOF -> {
                    break
                }
            }

            error_handler.report_error_at(tok.location, "Unexpeted token {}, expected {}", [tok, typ])
            lexer.skip_line()
            return tok
        }

        return lexer.next_token()
    }
}
